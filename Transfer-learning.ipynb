{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importowanie modułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafal\\.conda\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zbiory danych, modele, metody TL, hiperparametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'CIFAR10'\n",
    "]\n",
    "MODELS = [\n",
    "    models.alexnet(pretrained=True),\n",
    "    #models.vgg16(pretrained=True),\n",
    "    #models.resnet50(pretrained=True)\n",
    "]\n",
    "TL_METHODS = [\n",
    "    #'no_freezing',\n",
    "    #'classic_tl',\n",
    "    'fine_tuning'\n",
    "]\n",
    "\n",
    "# Hyperparmeters\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utworzenie Data setów i data loaderów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_and_dataloaders(dataset_name: str, model):\n",
    "\n",
    "    if isinstance(model, models.AlexNet) or isinstance(model, models.VGG) or isinstance(model, models.ResNet):\n",
    "        base_transforms = [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    else:\n",
    "        base_transforms = [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "\n",
    "    train_transform = transforms.Compose(base_transforms)\n",
    "    test_transform = transforms.Compose(base_transforms)\n",
    "\n",
    "    preprocessing = [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "    ]\n",
    "\n",
    "    train_transform.transforms.extend(preprocessing)\n",
    "\n",
    "    if dataset_name == 'CIFAR10':\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(\"Loaders created\")\n",
    "\n",
    "    return train_set, test_set, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przetwarzanie modeli do transfer learningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transfer_learning(model, tl_method):\n",
    "    if tl_method == 'classic_tl' or tl_method == 'fine_tuning':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if isinstance(model, models.AlexNet) or isinstance(model, models.VGG):\n",
    "            for param in model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif isinstance(model, models.ResNet):\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    elif tl_method == 'no_freezing':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "def customize_model(model, num_classes, tl_method):\n",
    "    if isinstance(model, models.AlexNet):\n",
    "        dropout = 0.5\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    elif isinstance(model, models.VGG):\n",
    "        dropout = 0.5\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    elif isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    print(\"Classifier for model has been changed.\")\n",
    "    apply_transfer_learning(model, tl_method)\n",
    "    print(\"Transfer learning method:\", tl_method, \"applied.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja mrożąca do naszej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freeze_percentage(model, percentage):\n",
    "    \"\"\"Freeze a percentage of the model's parameters based on gradient norms.\"\"\"\n",
    "    grad_params = [(param.grad.norm(), param) for param in model.parameters() if param.requires_grad and param.grad is not None]\n",
    "    grad_params.sort(key=lambda x: x[0])\n",
    "    num_params_to_freeze = int(percentage * len(grad_params))\n",
    "\n",
    "    for _, param in grad_params[:num_params_to_freeze]:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ustawienia fine-tuningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_fine_tuning(model, optimizer):\n",
    "    \"\"\"Adjust the model and optimizer for fine-tuning.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pętla treningowo walidacyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "Using the CIFAR10 dataset.\n",
      "Using the fine_tuning method\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loaders created\n",
      "Using the AlexNet model\n",
      "Classifier for model has been changed.\n",
      "Transfer learning method: fine_tuning applied.\n",
      "Training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▌        | 120/782 [04:43<26:06,  2.37s/it, loss=1.8] \n"
     ]
    }
   ],
   "source": [
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs, tl_method):\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train, total_train = 0, 0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}\") as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                \n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        val_loss, val_accuracy = evaluate_model(model, test_loader, criterion, running_loss, tl_method, optimizer)\n",
    "\n",
    "        history['train_loss'].append(running_loss / len(train_loader))\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    if tl_method == 'fine_tuning':\n",
    "        adjust_fine_tuning(model, optimizer)\n",
    "        print(\"Fine tuning applied. Learning rate: \", optimizer.lr)\n",
    "        tl_method = 'no_freezing'\n",
    "        fine_tuning_history = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs, tl_method)\n",
    "        history = {key: history[key] + fine_tuning_history[key] for key in history}\n",
    "\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, running_loss):\n",
    "    \"\"\"Evaluate the model and adjust learning rate if fine-tuning.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f'Train Loss: {running_loss/len(train_loader)}, '\n",
    "          f'Validation Loss: {val_loss/len(test_loader)}, '\n",
    "          f'Accuracy: {100 * correct_val / total_val}%')\n",
    "    \n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for id_dataset, dataset in enumerate(DATASETS):\n",
    "    print(f\"Using the {dataset} dataset.\")\n",
    "    \n",
    "\n",
    "    for id_tl_method, tl_method in enumerate(TL_METHODS):\n",
    "        print(f\"Using the {tl_method} method\")\n",
    "\n",
    "        for id_model, model in enumerate(MODELS):\n",
    "            train_set, test_set, train_loader, test_loader = create_datasets_and_dataloaders(dataset, model=model)\n",
    "            num_classes = len(train_set.classes)\n",
    "\n",
    "            print(f\"Using the {model.__class__.__name__} model\")\n",
    "            model = customize_model(model, num_classes, tl_method)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            model.to(device)\n",
    "\n",
    "            print(\"Training starts\")\n",
    "            train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, EPOCHS, tl_method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
