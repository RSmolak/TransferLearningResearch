{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importowanie modułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafal\\.conda\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms, datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zbiory danych, modele, metody TL, hiperparametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    'CIFAR10'\n",
    "]\n",
    "MODELS = [\n",
    "    models.alexnet(pretrained=True),\n",
    "    models.vgg16(pretrained=True),\n",
    "    models.resnet50(pretrained=True)\n",
    "]\n",
    "TL_METHODS = [\n",
    "    #'no_freezing',\n",
    "    #'classic_tl',\n",
    "    'fine_tuning'\n",
    "]\n",
    "\n",
    "# Hyperparmeters\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utworzenie Data setów i data loaderów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_and_dataloaders(dataset_name: str):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    if dataset == 'CIFAR10':\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_set, test_set, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przetwarzanie modeli do transfer learningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transfer_learning(model, tl_method):\n",
    "    if tl_method == 'classic_tl':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        if hasattr(model, 'classifier'):\n",
    "            for param in model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif hasattr(model, 'fc'):\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif tl_method == 'fine_tuning' or tl_method == 'no_freezing':\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "def customize_model(model, num_classes, tl_method):\n",
    "    if isinstance(model, models.AlexNet):\n",
    "        dropout = 0.5\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    elif isinstance(model, models.VGG):\n",
    "        dropout = 0.5\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    elif isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    print(\"Classifier for model has been changed.\")\n",
    "    apply_transfer_learning(model, tl_method)\n",
    "    print(\"Transfer learning method:\", tl_method, \"applied.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pętla ucząco walidacyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "def freeze_percentage(model, percentage):\n",
    "    # Create a list to store gradients and corresponding parameters\n",
    "    grad_params = []\n",
    "\n",
    "    # Collect gradients and parameters\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_params.append((param.grad.norm(), param))\n",
    "\n",
    "    # Sort parameters based on gradients (smaller gradients first)\n",
    "    grad_params.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Determine the number of parameters to freeze\n",
    "    num_params_to_freeze = int(percentage * len(grad_params))\n",
    "\n",
    "    # Freeze the parameters with the smallest gradients\n",
    "    for i in range(num_params_to_freeze):\n",
    "        grad_params[i][1].requires_grad = False\n",
    "\n",
    "for id_dataset, dataset in enumerate(DATASETS):\n",
    "    print(\"Using the\", dataset, \"dataset.\")\n",
    "\n",
    "    train_set, test_set, train_loader, test_loader = create_datasets_and_dataloaders(dataset)\n",
    "\n",
    "    num_classes = len(train_set.classes)\n",
    "\n",
    "    for id_tl_method, tl_method in enumerate(TL_METHODS):\n",
    "        print(\"Using the\", tl_method, 'method')\n",
    "\n",
    "        for id_model, model in enumerate(MODELS):\n",
    "            print(\"Using the\", model.__class__.__name__, \"model\")\n",
    "\n",
    "            # Model customization (to match the output size)\n",
    "            model = customize_model(model, num_classes, tl_method)\n",
    "\n",
    "            # Loss Function and Optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # Training and Validation\n",
    "            print(\"Training starts\")\n",
    "            for epoch in range(EPOCHS):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                num_batches = len(train_set.data) // BATCH_SIZE\n",
    "                for batch, (images, labels) in enumerate(train_loader):\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    print(\"\\rEpoch:\", epoch, \"Batch:\", batch+1, '/', num_batches, end =' ')\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_loader:\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, '\n",
    "                    f'Validation Loss: {val_loss/len(test_loader)}, '\n",
    "                    f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "                # Fine-tuning based on tl_method\n",
    "                if tl_method == 'fine_tuning':\n",
    "                    freeze_percentage(model, percentage=0.2)  # Adjust the percentage as needed\n",
    "                    # Unfreeze all layers for fine-tuning\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    # Adjust learning rate for fine-tuning\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
